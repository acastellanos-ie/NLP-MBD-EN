{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "practice_solution",
      "provenance": [],
      "collapsed_sections": [
        "LqYDlxVx_9O8",
        "rGRSIwI8_9Py",
        "KXkLmX5I8xGZ",
        "ncGqMmK68xGk"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "jekyll": {
      "keywords": "fastai",
      "summary": "Application to NLP, including ULMFiT fine-tuning",
      "title": "text"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBD-EN/blob/main/text_classification_practice_dl/practice_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44HVLnqAxYAk"
      },
      "source": [
        "# Google Colab Configuration\n",
        "\n",
        "**Execute this steps to configure the Google Colab environment in order to execute this notebook. It is not required if you are executing it locally and you have properly configured your local environment according to what explained in the Github Repository.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "957PYTqES-7h"
      },
      "outputs": [],
      "source": [
        "# @title Colab Setup\n",
        "\n",
        "repository_name = \"NLP-MBD-EN\"\n",
        "repository_url = 'https://github.com/acastellanos-ie/' + repository_name\n",
        "\n",
        "print(\"### Cloning the Repository ###\")\n",
        "! git clone $repository_url\n",
        "print()\n",
        "\n",
        "print(\"### Installing requirements ###\")\n",
        "! pip3 install -Uqqr $repository_name/text_classification_practice_dl/requirements.txt\n",
        "\n",
        "%cd $repository_name/text_classification_practice_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything is set up properly! You can now move on to the practice code.\n",
        "\n",
        "I do recommend you to restart the environment at this point to ensure that Google Colab is aware of the installed packages and their configuration (see image below).\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "i_TXNtLWgA7p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKqBBIxf03lS"
      },
      "source": [
        "Ensure that you have the GPU runtime activated:\n",
        "\n",
        "![](https://miro.medium.com/max/3006/1*vOkqNhJNl1204kOhqq59zA.png)\n",
        "\n",
        "Now you have everything you need to execute the code in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAHXbPsD_9Oq"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Matplotlib conf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Seaborn conf\n",
        "import seaborn as sns\n",
        "sns.set_palette(sns.color_palette(\"seismic\"))\n",
        "\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import operator\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l56xpzD4YsCp"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "In this practice we will address a text classification problem: we will try to predict whether a given tweet is about a real disaster or not. In particular I propose you the scenario presented at the [Real or Not? NLP with Disaster Tweets Kaggle Competition.](https://www.kaggle.com/c/nlp-getting-started/leaderboard).\n",
        "\n",
        "To that, I am going to follow 2 different approaches:\n",
        "- Apply traditional text classification methodologies based on \"simple\" Machine Learning models\n",
        "- Apply state-of-the-art deep learning models\n",
        "\n",
        "I have decided to give Deep Learning a try by applying the classification models explained in class.\n",
        "However, the dataset we have at our disposal includes \"only\" 65K documents. We all know that to apply Deep Learning you need tons of data and weeks of training, right?\n",
        "\n",
        "Well, this in not true, or at least, this is not anymore completely true thanks to **Transfer Learning**. If you are training a huge model from scratch, you do need a lot of data and GPU time. Lucklily for us, these huge models are already pre-trained for many languages using large datasets (e.g. Wikipedia). All we have to do is to adapt these models to our particular domain. 65K documents are not enough to learn English, but they are definitely enough to learn the nuances of the dataset.\n",
        "\n",
        "Cool, now we have a model adapted to the specific language in my dataset but the business case was to create a classifier. Why am I even doing all of this? In the traditional ML methodologies we were feeding our algorithms with a rather simple representation of our textual contents (The TF-IDF vectors). Now, thanks to the fine-tune pre-trained models, I can feed my classifier with a much more detailed and accurate representation of the input textual content. Therefore, it should be easier to the classifier to better categorize the textual content.\n",
        "\n",
        "\n",
        "Summing up, my strategy will be:\n",
        "\n",
        "1.   Use a pre-trained English Language Model trained over a large dataset as starting point.\n",
        "2.   Adapt this language model to our domain. To that end, I will retrain the model to learn the particular aspects of the dataset.\n",
        "3.   Create a Machine Learning Classifier on top of the dataset language model\n",
        "\n",
        "\n",
        "**Side note: I have trained this model on Google Colab to be able to use a GPU.For this reason, I did not include anything about the configuration of the GPU or the installation of the required libraries. To execute it I do recommend you to update it to Google Colab**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqYDlxVx_9O8"
      },
      "source": [
        "# STEP 1: Loading and preprocessing the data\n",
        "\n",
        "Before starting to play with the DeepLearning models we need to read, inspect and clean (if needed) the dataset.\n",
        "\n",
        "The dataset is provided in 2 different CSV files (one for training and another one for test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8pPwgW58xG6"
      },
      "source": [
        "training_df = pd.read_csv('nlp_disaster/data/train.csv', sep=',', index_col=0)\n",
        "training_df = training_df.sample(frac=1).reset_index(drop=True)\n",
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zONg03Yf_9O-"
      },
      "source": [
        "As you can see, the training set contains the dependent variable (1 or 0, real or not) as well as some independent variables: the actual text of the tweet and some of the metadata related"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOx1Jmd59Mon"
      },
      "source": [
        "test_df = pd.read_csv('nlp_disaster/data/test.csv', sep=',', index_col=0)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkvr3QxO_9PB"
      },
      "source": [
        "The test set contains the same independent variables, but no label whatsoever. This is the common scenario proposed by Kaggle. You get a training set to create your models and then you have to use these models to predict over the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhgXVfXj_9PC"
      },
      "source": [
        "## Inspecting the data\n",
        "\n",
        "In this step I will try to make sense of the kind of data that we have.\n",
        "\n",
        "### Class distribution\n",
        "\n",
        "The first thing I will do is to inspect the distribution of the target variable to check if we have an imbalace problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJXZCZcV_9PC"
      },
      "source": [
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# Build your order\n",
        "order = [lab for lab, _ in sorted(Counter(training_df.target).items(), key=lambda x: -x[1])]\n",
        "\n",
        "# Now tell countplot to use 'target' as both x and hue\n",
        "sns.countplot(\n",
        "    x=\"target\",\n",
        "    hue=\"target\",\n",
        "    data=training_df,\n",
        "    order=order,\n",
        "    palette=\"seismic\",\n",
        "    dodge=False,      # so you don't get side-by-side bars\n",
        "    legend=False      # hides the pointless legend\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ4uReLi_9PF"
      },
      "source": [
        "As seen in the figure, There are a slight difference between the number of tweets at each class but nothing to worry about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO7Xb1XB_9PG"
      },
      "source": [
        "### Metadata\n",
        "\n",
        "Beyond the textual content, we have 2 pieces of information:\n",
        "\n",
        " - keyword: the keyword used as query to retrieve the tweets\n",
        " - location: Location of the user posting the tweet\n",
        "\n",
        "Regarding the **keywords**, some of them can be strongly related to a given category, but they can be also highly ambiguous as you can see in the following figure (I used the code in this kernel to create it: https://www.kaggle.com/grantgasser/eda-naive-bayes-bert-glove-fasttext-nn).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "j9jeQa1J_9PG",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Copy the dataframes to be able to come back to the original version\n",
        "df_train = training_df.copy()\n",
        "df_test = test_df.copy()\n",
        "\n",
        "df_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n",
        "\n",
        "fig = plt.figure(figsize=(8, 72), dpi=100)\n",
        "\n",
        "sns.countplot(y=df_train.sort_values(by='target_mean', ascending=False)['keyword'],\n",
        "              hue=df_train.sort_values(by='target_mean', ascending=False)['target'], palette='seismic')\n",
        "\n",
        "plt.tick_params(axis='x', labelsize=15)\n",
        "plt.tick_params(axis='y', labelsize=12)\n",
        "plt.legend(loc=1)\n",
        "plt.title('Target Distribution in Keywords')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "df_train.drop(columns=['target_mean'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdo-H843_9PH"
      },
      "source": [
        "Given that the keywords are already included in the textual content (the tweets have been retrieved because they include some of the keywords), I have decided to discard this information as I expect that we can learn the importance of the keywords as individual tokens in the textual content.\n",
        "\n",
        "A future work that I recommend you to consider is to create a classifier just based on the keywords and compare its performance to the most advanced ones that I am proposing in this solution. If a Classifier simply based on the keywords is good enough, perhaps you can stick to it, thus avoiding the complexities of dealing with textual information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoJhbq5k_9PJ"
      },
      "source": [
        "Let's analyze the **location**.\n",
        "\n",
        "The first aspect to consider is that it has many null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TTxs1n3_9PL"
      },
      "source": [
        "print(\"% of null columns =\", (len(training_df.location) - training_df.location.count())/len(training_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM1EAVJp_9PL"
      },
      "source": [
        "This is a common features of any twitter dataset. For privacy reasons, many people prefer to keep their locations to themselves. This could change in the case of a disaster, you may want to share your location to give a better understanding of the disaster. I am not going to focus on this idea and, again, I encourage you to test it and check if you can further improve your models.\n",
        "\n",
        "Summing up, I will only focus on the textual information (`text` column) of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmoRMBWZ_9PM"
      },
      "source": [
        "### Textual Data\n",
        "\n",
        "At this point we have to options:\n",
        "\n",
        "- Focus just on the actual textual content by applying the NLP methodologies addressed in class for text classification\n",
        "- Increase the information in the dataset by feature-engineer some new features derived from the textual content (e.g., lenght of the tweets, how many stopwords in the tweets, how many urls...).\n",
        "    \n",
        "To further analyze if this idea makes sense, I have used the code in this kernel (https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert) to create some of these features and plot them against the target variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t93N970_9PN"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "# word_count\n",
        "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
        "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# unique_word_count\n",
        "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
        "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "# stop_word_count\n",
        "df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in nltk.corpus.stopwords.words(\"english\")]))\n",
        "df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in nltk.corpus.stopwords.words(\"english\")]))\n",
        "\n",
        "# url_count\n",
        "df_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "df_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "# mean_word_length\n",
        "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "# char_count\n",
        "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
        "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# punctuation_count\n",
        "df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "\n",
        "# hashtag_count\n",
        "df_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "\n",
        "# mention_count\n",
        "df_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqnC6nAE_9PP"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n",
        "                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n",
        "DISASTER_TWEETS = df_train['target'] == 1\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n",
        "\n",
        "for i, feature in enumerate(METAFEATURES):\n",
        "    sns.distplot(df_train.loc[~DISASTER_TWEETS][feature], label='Not Disaster', ax=axes[i][0], color='green')\n",
        "    sns.distplot(df_train.loc[DISASTER_TWEETS][feature], label='Disaster', ax=axes[i][0], color='red')\n",
        "\n",
        "    sns.distplot(df_train[feature], label='Training', ax=axes[i][1], color='green')\n",
        "    sns.distplot(df_test[feature], label='Test', ax=axes[i][1], color='red')\n",
        "\n",
        "    for j in range(2):\n",
        "        axes[i][j].set_xlabel('')\n",
        "        axes[i][j].tick_params(axis='x', labelsize=12)\n",
        "        axes[i][j].tick_params(axis='y', labelsize=12)\n",
        "        axes[i][j].legend()\n",
        "\n",
        "    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n",
        "    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5vPTt6b_9PS"
      },
      "source": [
        "These figures shows the comparison between the class `disaster` and `not disaster` in the training set (on the left) and the comparison between the training and test datasets (on the right) for the different derived features.\n",
        "\n",
        "In general terms, they do not provide any interesting insight. The distribution of all of these new features are basically the same in both classes and datasets. Consequently, I do not expect them to be of any help for creating the classification models (or at least I do not expect their impact to be large enough to motivate their use). At a closer inspection, the length of the tweets seems to be slightly different between the classes. In this sense, you can test this aspect by including these new feature in your models and check if you can improve your final model.\n",
        "\n",
        "Anyhow, I have decided to just use the textual information without any new feature to create my models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUGMecl4_9PT"
      },
      "source": [
        "**Let us now inspect the actual text of the tweets belonging to each class.**\n",
        "\n",
        "In the following figure we plot the most common words in each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "aTIFvU3X_9PT",
        "scrolled": true
      },
      "source": [
        "def plot_most_common_features(text_col, target_col, n_features=50):\n",
        "    from matplotlib import interactive\n",
        "\n",
        "    df = pd.DataFrame({\"text\": text_col, \"CLASS\": target_col})\n",
        "    grouped = df.groupby([\"CLASS\"]).apply(lambda x: x[\"text\"].sum())\n",
        "    grouped_df = pd.DataFrame({\"CLASS\": grouped.index, \"text\": grouped.values})\n",
        "\n",
        "    from nltk.tokenize import WhitespaceTokenizer\n",
        "    tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "    for ii, text in enumerate(grouped_df.text):\n",
        "        pd.DataFrame(tokenizer.tokenize(text)).apply(pd.value_counts).head(n_features).plot(kind=\"bar\", cmap=plt.cm.seismic, figsize=(20,5))\n",
        "        plt.title(grouped_df.CLASS[ii], fontsize=20)\n",
        "        plt.xticks(fontsize=15)\n",
        "        plt.legend([])\n",
        "        interactive(True)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oifDiCp_9PU"
      },
      "source": [
        "plot_most_common_features(training_df.text, training_df.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM_wePFa_9PV"
      },
      "source": [
        "Well, this first inspection has been quite a dissapointment.\n",
        "\n",
        "By plotting the most repeated words in each class we expected to see the words \"most representative\" of each class. If they were different enough, this will indicate us that we can use them to easily identify the classes.\n",
        "\n",
        "However, what we see is the same set of uninformative words for both classes. What we are seeing are mostly **stopwords** which are not related to the class and might affect the final classifier. It seems that we will have to remove them latter.\n",
        "\n",
        "Another issue that we can detect with this inspection is that we have words like `A` and `a` which are the same but are considered as a different term because of the capitalization. It shows that we will have to **normalize** the words.\n",
        "\n",
        "In addition, we find different forms of the same verb: `are`, `be`, `is`. If you remember for class we have two methodologies to extract the basic form of a word: **stemming and lemmatization**. Stemming will not resolve this problem. As seen in class it finds the basic forms (stems) by chunking the words. In contrast, by lemmatizing the words we will find the basic form of the words (`be` for the words `are`, `be` and `is`).\n",
        "In any case, it may be worthy to try both approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb9XLKX5_9PX"
      },
      "source": [
        "### Text Processing and Data Preparation\n",
        "\n",
        "Let's try to address the aforementioned issues. We will start with a basic Natural Language Pre-processing:\n",
        "\n",
        " - Extract English Words\n",
        " - Stem words to avoid over counting same meaning words\n",
        " - Removing stop words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXGO-L81_9PX"
      },
      "source": [
        "I will join training and test datasets to apply the same process to both"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvmEc_le_9PY"
      },
      "source": [
        "dataset = pd.concat([training_df,test_df], sort=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxkuVUYs_9PY"
      },
      "source": [
        "To encapsulate this process and facilitate its application, we create a function `process_text` which encloses all the text processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          6
        ],
        "id": "hJLsf4uI_9PZ"
      },
      "source": [
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "\n",
        "def process_text(raw_text):\n",
        "\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_text)\n",
        "    words = letters_only.lower().split()\n",
        "\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    not_stop_words = [w for w in words if not w in stops]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed = [stemmer.stem(word) for word in not_stop_words]\n",
        "\n",
        "    return( \" \".join( stemmed ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk_d_GnG_9Pa"
      },
      "source": [
        "We now apply this function to the textual content.\n",
        "Just in case we need the raw text later, we will create new columns to store the processed text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMNMDfRG_9Pa"
      },
      "source": [
        "dataset['clean_text'] = dataset['text'].apply(lambda x: process_text(x))\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3rs3EqM_9Pb"
      },
      "source": [
        "plot_most_common_features(dataset.clean_text, dataset.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19u9TTpR_9Pd"
      },
      "source": [
        "It looks much better now!\n",
        "We have removed all the stopwords and the terms that we see now are much more representative (e.g., `bomb`, `crash` among the tweets about actual disasters).\n",
        "\n",
        "However, we also see another meaningless words (e.g., `co`, `http`) in both classes. This kind of terms are known as domain-stopwords. I.e., they are not stopwords of the English language but they are very common terms in our particular domain (e.g., HTML keywords). As I cannot see how they can inform about the category of the tweet, I have decided to remove them.\n",
        "\n",
        "To that end, I am going to modify the `process_text` to clean all of the possible domain stopwords that I can find in the textual content of the tweets. I have found the following function in this Kaggle kernel: https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpP7yTYu_9Pe",
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "def clean(tweet):\n",
        "\n",
        "    # Special characters\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)\n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "\n",
        "\n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "\n",
        "    # Typos, slang and informal abbreviations\n",
        "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
        "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
        "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
        "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
        "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
        "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
        "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
        "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
        "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
        "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
        "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
        "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
        "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
        "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
        "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
        "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)\n",
        "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
        "\n",
        "    # Hashtags and usernames\n",
        "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
        "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
        "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet)\n",
        "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)\n",
        "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
        "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
        "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
        "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
        "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
        "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
        "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
        "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
        "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
        "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
        "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
        "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
        "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
        "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
        "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
        "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
        "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
        "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
        "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
        "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
        "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
        "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
        "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
        "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
        "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
        "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
        "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
        "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
        "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
        "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
        "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
        "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
        "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
        "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
        "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
        "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
        "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
        "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
        "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
        "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
        "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
        "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
        "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
        "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
        "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
        "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
        "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
        "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
        "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
        "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
        "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
        "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
        "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
        "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
        "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
        "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
        "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
        "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
        "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)\n",
        "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
        "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
        "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
        "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
        "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
        "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
        "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
        "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
        "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
        "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
        "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
        "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
        "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
        "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
        "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
        "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
        "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
        "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
        "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
        "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
        "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
        "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
        "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
        "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
        "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
        "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
        "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
        "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
        "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
        "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
        "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
        "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
        "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
        "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
        "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
        "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
        "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
        "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
        "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
        "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
        "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
        "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
        "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
        "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
        "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
        "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
        "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
        "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
        "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
        "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
        "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
        "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
        "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
        "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
        "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
        "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
        "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
        "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
        "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
        "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
        "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
        "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
        "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
        "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
        "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
        "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
        "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
        "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
        "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
        "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
        "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
        "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
        "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
        "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
        "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
        "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
        "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
        "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
        "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
        "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
        "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
        "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
        "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
        "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
        "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
        "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
        "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
        "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
        "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
        "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
        "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
        "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
        "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
        "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
        "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
        "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
        "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
        "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
        "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
        "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
        "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
        "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
        "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
        "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
        "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
        "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
        "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
        "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
        "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
        "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
        "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
        "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
        "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
        "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
        "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
        "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
        "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
        "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
        "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
        "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
        "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
        "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
        "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
        "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
        "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
        "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
        "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
        "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
        "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
        "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
        "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
        "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
        "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
        "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
        "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
        "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
        "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
        "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
        "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
        "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
        "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
        "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
        "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
        "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
        "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
        "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
        "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
        "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
        "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
        "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
        "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
        "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
        "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
        "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
        "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
        "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
        "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
        "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
        "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
        "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
        "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
        "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
        "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
        "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
        "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
        "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
        "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
        "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
        "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
        "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
        "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
        "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
        "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
        "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
        "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
        "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
        "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
        "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
        "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
        "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
        "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
        "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
        "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
        "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
        "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
        "    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n",
        "    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n",
        "    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n",
        "    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n",
        "    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n",
        "    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n",
        "    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n",
        "    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n",
        "    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n",
        "    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n",
        "    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n",
        "    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n",
        "    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n",
        "    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n",
        "    tweet = re.sub(r\"andword\", \"and word\", tweet)\n",
        "    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n",
        "    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n",
        "    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n",
        "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
        "    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n",
        "    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n",
        "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
        "    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n",
        "    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n",
        "    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n",
        "    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n",
        "    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n",
        "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
        "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
        "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
        "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
        "    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n",
        "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
        "    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n",
        "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
        "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
        "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
        "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
        "    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n",
        "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
        "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
        "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
        "    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n",
        "    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n",
        "    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n",
        "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
        "    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n",
        "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
        "    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n",
        "    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n",
        "    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n",
        "    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n",
        "    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n",
        "    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n",
        "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
        "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
        "    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n",
        "    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
        "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
        "    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n",
        "    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n",
        "    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n",
        "    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n",
        "    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n",
        "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
        "    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n",
        "    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n",
        "    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n",
        "    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n",
        "    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n",
        "    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n",
        "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
        "    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n",
        "    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n",
        "    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n",
        "    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n",
        "    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n",
        "    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n",
        "    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n",
        "    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n",
        "    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n",
        "    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n",
        "    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n",
        "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
        "    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n",
        "    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n",
        "    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n",
        "    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n",
        "    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n",
        "    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n",
        "    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n",
        "    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n",
        "    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n",
        "    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n",
        "    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n",
        "    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n",
        "    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n",
        "    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n",
        "    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n",
        "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
        "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
        "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
        "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
        "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
        "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
        "    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n",
        "    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n",
        "    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n",
        "    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
        "    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n",
        "    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n",
        "    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n",
        "    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n",
        "    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n",
        "    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n",
        "    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n",
        "    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n",
        "    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n",
        "    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n",
        "    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n",
        "    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n",
        "    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n",
        "    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n",
        "    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n",
        "    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n",
        "    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n",
        "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
        "    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
        "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
        "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
        "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
        "    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n",
        "    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n",
        "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
        "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
        "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
        "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
        "    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n",
        "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
        "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
        "    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n",
        "    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n",
        "    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n",
        "    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n",
        "    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n",
        "    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n",
        "    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n",
        "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
        "    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n",
        "    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n",
        "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
        "    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n",
        "    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n",
        "    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n",
        "    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n",
        "    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n",
        "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
        "    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n",
        "    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n",
        "    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n",
        "    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n",
        "    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n",
        "    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n",
        "    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n",
        "    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n",
        "    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n",
        "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
        "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
        "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
        "    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n",
        "    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n",
        "    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n",
        "    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n",
        "    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n",
        "    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n",
        "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
        "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
        "    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n",
        "    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n",
        "    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n",
        "    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n",
        "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
        "    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n",
        "    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n",
        "    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n",
        "    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n",
        "    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n",
        "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
        "    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n",
        "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
        "    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n",
        "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
        "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
        "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
        "    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n",
        "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
        "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
        "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
        "    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n",
        "    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n",
        "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
        "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
        "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
        "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
        "    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n",
        "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
        "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
        "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
        "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
        "    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n",
        "    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n",
        "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
        "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
        "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
        "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
        "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
        "    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n",
        "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
        "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
        "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
        "    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n",
        "    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n",
        "    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n",
        "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
        "    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n",
        "    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n",
        "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
        "    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n",
        "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
        "    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n",
        "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
        "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n",
        "    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n",
        "    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n",
        "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
        "    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n",
        "    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n",
        "    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n",
        "    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
        "    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n",
        "    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n",
        "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
        "    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n",
        "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
        "    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n",
        "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
        "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
        "    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n",
        "    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n",
        "    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n",
        "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
        "    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n",
        "    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n",
        "    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n",
        "    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n",
        "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
        "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
        "    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n",
        "    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n",
        "    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n",
        "    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n",
        "    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n",
        "    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n",
        "    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n",
        "    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n",
        "    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n",
        "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
        "    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n",
        "    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n",
        "    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n",
        "    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n",
        "    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n",
        "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
        "    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n",
        "    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n",
        "    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n",
        "    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n",
        "    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n",
        "    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n",
        "    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n",
        "    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n",
        "    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n",
        "    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n",
        "    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n",
        "    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n",
        "    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n",
        "    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n",
        "    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n",
        "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
        "\n",
        "    # Urls\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "\n",
        "    # Words with punctuations and special characters\n",
        "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
        "    for p in punctuations:\n",
        "        tweet = tweet.replace(p, f' {p} ')\n",
        "\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJBWhrQd_9Pu"
      },
      "source": [
        "def process_text(raw_text):\n",
        "\n",
        "    raw_text = clean(raw_text)\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_text)\n",
        "    words = letters_only.lower().split()\n",
        "\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    not_stop_words = [w for w in words if not w in stops]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed = [stemmer.stem(word) for word in not_stop_words]\n",
        "\n",
        "    return( \" \".join( stemmed ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7bOKCPI_9Pv"
      },
      "source": [
        "dataset['clean_text'] = dataset['text'].apply(lambda x: process_text(x))\n",
        "plot_most_common_features(dataset.clean_text, dataset.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuJUx612_9Pw"
      },
      "source": [
        "Right, now we only see what appears to be relevant terms among the most common ones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho5YLdn-_9Pw"
      },
      "source": [
        "We now split the dataset again for the creation and evaluation of the models into training and test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhRkP0ex_9Py"
      },
      "source": [
        "X_train = dataset[0:len(training_df)][[\"clean_text\"]]\n",
        "y_train = dataset[0:len(training_df)][[\"target\"]]\n",
        "X_test = dataset[len(training_df):len(dataset)][[\"clean_text\"]]\n",
        "y_test = dataset[len(training_df):len(dataset)][[\"target\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGRSIwI8_9Py"
      },
      "source": [
        "# STEP 2: Machine Learning Models\n",
        "\n",
        "Now that we have a clean version of the dataset we can move to the training of ML models.\n",
        "\n",
        "Before starting the training process, we need to take care of some aspects related to the data format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3guUiRTc_9Pz"
      },
      "source": [
        "Labels are in string format. It is preferred to have them numerically encoded. To that end, sklearn provides a `LabelEncoder` to facilitate this encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GypjUCc4_9Pz"
      },
      "source": [
        "# Hot encoding for the labels\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_train.target.values)\n",
        "target_labels = le.classes_\n",
        "encoded_y_train = le.transform(y_train.target.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C_ZMtCc_9P0"
      },
      "source": [
        "\n",
        "We then need to convert the textual content into numerical feature vectors (i.e., Vector Space Model), applying the bags of words representation:\n",
        "\n",
        " - Assign an id to each word of the training set.\n",
        " - For each document `d`, count the number of occurrences of each word `w` and store it in `X[i, j]` as the value of feature `j` where `j` is the index of `w` in the dictionary\n",
        "\n",
        "Sklearn has also some useful functions for this textual processing that we will make use of."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S7F_n-V_9P0"
      },
      "source": [
        "We are going to use the `CountVectorizer` for creating the feature vectors and the `TfidfTransformer` which returns the Document-Term Matrix with the terms weighted by means of their TF-IDF score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHCHVhDy_9P2"
      },
      "source": [
        "count_vect = CountVectorizer(analyzer = \"word\")\n",
        "train_features = count_vect.fit_transform(X_train['clean_text'])\n",
        "test_features = count_vect.transform(X_test['clean_text'])\n",
        "\n",
        "tfidf = TfidfTransformer(norm=\"l2\")\n",
        "train_text_tfidf_features = tfidf.fit_transform(train_features)\n",
        "test_text_tfidf_features = tfidf.fit_transform(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC5-zhl1_9P3"
      },
      "source": [
        "Now we have our dataset in the needed format that will allow us to create our baseline models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-cZ0zwL_9P3"
      },
      "source": [
        "To train, evaluate and compare the models I've created a function `train_and_evaluate_classifier` that takes the training data, a classifier definition and a grid to optimize the classifier to train a predictive model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "VvMgboIa_9P4"
      },
      "source": [
        "def train_and_evaluate_classifier(X, yt, estimator, grid):\n",
        "    \"\"\"Train and Evaluate a estimator (defined as input parameter) on the given labeled data using accuracy.\"\"\"\n",
        "\n",
        "    # Cross validation\n",
        "    from sklearn.model_selection import ShuffleSplit\n",
        "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
        "\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=estimator, cv=cv,  param_grid=grid, error_score=0.0, n_jobs = -1, verbose = 0)\n",
        "\n",
        "    # Train the model over and tune the parameters\n",
        "    print(\"Training model\")\n",
        "    grid_search.fit(X, yt)\n",
        "\n",
        "    # CV-score\n",
        "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "    if len(grid) > 0:\n",
        "        print(\"Best parameters set:\")\n",
        "        best_parameters = grid_search.best_estimator_.get_params()\n",
        "\n",
        "    return grid_search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "8lTveatQ_9P4"
      },
      "source": [
        "## Approach #1: Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "HcOh9E8U_9QD"
      },
      "source": [
        "Let's apply Naïve Bayes for the textual classification.\n",
        "Remember for class that in order to predict the class of a given document we compute the \"Maximum Apriori Probability\":\n",
        "\n",
        "\\begin{equation*}\n",
        "MAP   = max({P(c_1|w_1,...w_n), P(c_2|w_1,...,w_n)})\n",
        "\\end{equation*}\n",
        "\n",
        "To compute $P(c|w_1,...w_n)$, we will use the Bayes theorem:\n",
        "\n",
        "\\begin{equation*}\n",
        "P(c|w_1,...w_n) = \\frac{P(w_1,...,w_n|c)P(c)}{P(w_1,...,w_n)}\n",
        "\\end{equation*}\n",
        "\n",
        "Being \"naive\", we can assume that w_1, ..., w_n are independent among them. Therefore:\n",
        "\n",
        "\\begin{equation*}\n",
        "P(w_1,...,w_n|c) = P(w_1|c)P(w_2|c) ... P(w_n|c)\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "YCZIg63c_9QD"
      },
      "source": [
        "We use the column `clean_text` to create the features through the count vectorizer object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "IaLfFajg_9QE"
      },
      "source": [
        "nb_text_cls = train_and_evaluate_classifier(train_text_tfidf_features, encoded_y_train, MultinomialNB(), {})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "5JGb9FNF_9QE"
      },
      "source": [
        "80% of Accuracy is a strong baseline. In this solution I am going to explore more solutions to further enhance these results. Nevertheless, if you are trying to create a production system, it could be enough. As you could have seen, it is quite easy and fast to train a NB classifier.\n",
        "\n",
        "Only if you actually need a state of the art classifier you could delve into the following methodologies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "2wKwDJ3j_9QF"
      },
      "source": [
        "## Approach 2: SVM\n",
        "\n",
        "As seen in class, SVM is better suited for text classification, providing a more accurate model. By making use of sklearn, we are going to implement a SVM classifier and then apply it to detect fake news and check if we can improve our baseline Naïve Bayes model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "zDlx23Qq_9QH"
      },
      "source": [
        "We now make use of the `train_and_evaluate_classifier` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "t6ZXAN7N_9QH"
      },
      "source": [
        "# SVM model\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_grid = [\n",
        "  {'C': [0.01, 0.1, 1], 'kernel': ['linear']},\n",
        "  {'C': [1, 10, 100, 1000], 'gamma': ['scale', 'auto'], 'kernel': ['rbf']},\n",
        " ]\n",
        "\n",
        "svm_cls = train_and_evaluate_classifier(train_text_tfidf_features, encoded_y_train, SVC(), svm_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "IMrnkkcm_9QI"
      },
      "source": [
        "As expected, better than our baseline model based on NB.\n",
        "\n",
        "In general, SVM is a more suited algorithm for text classification than Naive Bayes. It's up to you to decided if the increase in complexity is justified by the increase in performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "PZBn5cR4_9QI"
      },
      "source": [
        "## Approach 3: MaxEnt Classifiers\n",
        "\n",
        "In class we studied the maxent classifiers. They are probabilistic models especially suited for text classification since they do not assume any independence between the textual contents.\n",
        "They are expected to offer better results than simple NB without the complexity of SVM. Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "3loVeHkK_9QJ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "max_ent_grid= {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
        "max_ent_cls = train_and_evaluate_classifier(train_text_tfidf_features, encoded_y_train, LogisticRegression(), max_ent_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Hg3uce7a_9QL"
      },
      "source": [
        "Results are close to those offered by NB.\n",
        "\n",
        "Usually this is not the case: MaxEnt usually works way better for text classification than NB. However, we have to consider again the special scenario in which we are: Twitter.\n",
        "\n",
        "In Twitter, the textual content is very scarce (we have only a bunch of words). In this scenario, even simple approaches like NB are able to capture the overall meaning of a textual content. If you think about it, in order to understand if some tweet is about an actual disaster, many times is just enough to detect a couple of highly words (fire, California), instead of having to deeply understande the textual content of the tweets. In this sense, the independence assumption in which NB relies is not so wrong anymore, thus offering a similar result than more \"intelligent\" approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiGI0XMJ_9QL"
      },
      "source": [
        "Summing up, with traditional ML we have been able to achieve a performance around 80%. That's not bad, however I would like to give a try to some of the most advanced classification methodologies, based on Deep Learning.\n",
        "\n",
        "As you will see, thanks to some libraries it is not so difficult/scary as it might seem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQRO99ri_9QM"
      },
      "source": [
        "# STEP 3: Recurrent Neural Nets (RNN)\n",
        "\n",
        "In class we have discussed about this kind of Deep Learning models. RNNs apply a DL architecture focused on modeling sequential information. In particular, Long Sort Term Memory Networks, a subtype of RNNs, are especially well suited for text. Since they are able to capture short term as well as long term relationships, commonly present in textual information.\n",
        "\n",
        "In more detail we are going to make use of the [AWD-LSTM](https://arxiv.org/abs/1708.02182) model: one of the top-performing approaches in the text classification literature. Do not worry, you do not need to implement by yourself the model in the paper. Luckily for us, their authors have already implemented it in the fast.ai library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXkLmX5I8xGZ"
      },
      "source": [
        "## Fast AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7eq6viH8xGi"
      },
      "source": [
        "To facilitate the training of this deep learning approach, I will make use of the [fast.ai library](https://www.fast.ai/). In particular, the [`text`](https://docs.fast.ai/text.html) module of the fast.ai library contains all the necessary functions. Specifically:\n",
        "\n",
        "- [`text.transform`](https://docs.fast.ai/text.transform.html#text.transform) contains all the scripts to preprocess the data, from raw text to token ids,\n",
        "- [`text.data`](https://docs.fast.ai/text.data.html#text.data) contains the definition of [`TextDataBunch`](https://docs.fast.ai/text.data.html#TextDataBunch), which is the main class we need in NLP,\n",
        "- [`text.learner`](https://docs.fast.ai/text.learner.html#text.learner) contains helper functions to quickly create a language model or an text classifier.\n",
        "\n",
        "For more information about the library and how to apply it to text classification, please refer to the related [chapter](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) of the fast.ai book\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncGqMmK68xGk"
      },
      "source": [
        "\n",
        "## Training a classifier model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbbEDziH8xGl"
      },
      "source": [
        "To create my model I am going to apply the following steps:\n",
        "\n",
        "1. Fine-tuning an [AWD-LSTM](https://arxiv.org/abs/1708.02182) model to create a language model based on our data.\n",
        "1. Building a classifier based on the learned language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrXdKN9f8xGn"
      },
      "source": [
        "### Reading and viewing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4AmXyT78xGo"
      },
      "source": [
        "First let's import everything we need for text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X60QVaOO8xGq"
      },
      "source": [
        "from fastai.basics import *\n",
        "from fastai.callback.all import *\n",
        "from fastai.text.all import *\n",
        "\n",
        "import torch\n",
        "from fastcore.foundation import L\n",
        "\n",
        "# allow torch.load to unpickle L\n",
        "torch.serialization.add_safe_globals([L])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDSdT0CS8xG_"
      },
      "source": [
        "### Getting your data ready for modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUCCkCV68xHF"
      },
      "source": [
        "In the same way that we needed to use the Vectorizers in sklearn to create the docment-term matrix to fed the machine learning models, we need to properly format the input textual data so we can feed the DeepLearning model.\n",
        "\n",
        "fast.ai provides some helpful loaders to format the input data to the format required by the Deep Learning Model.\n",
        "\n",
        "The main structure we need to create is the [`DataBunch`](https://docs.fast.ai/basic_data.html#DataBunch) that creates the training and validation sets by grabbing the textual data from the required columns.\n",
        "\n",
        "Here we'll use the method <code>from_df</code> of the [`TextLMDataBunch`](https://docs.fast.ai/text.data.html#TextLMDataBunch) (to get the data ready for the fine-tuning of the language model) and [`TextClasDataBunch`](https://docs.fast.ai/text.data.html#TextClasDataBunch) (to get the data ready for the classification step) classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DPll640ZKye"
      },
      "source": [
        "# Language model data\n",
        "df_all = pd.concat([training_df, test_df])\n",
        "\n",
        "dls_lm = DataBlock(\n",
        "    blocks=TextBlock.from_df('text', is_lm=True),\n",
        "    get_x=ColReader('text'),\n",
        "    splitter=RandomSplitter(0.1)\n",
        ").dataloaders(df_all, bs=64, seq_len=40)\n",
        "\n",
        "dls_lm.show_batch(max_n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Ybp1S__9QY"
      },
      "source": [
        "We are not applying any data cleaning given that we need the original text in the tweet.\n",
        "\n",
        "Think about it, we are trying to update the pre-trained language model with the data in our dataset. LMs are trained by asking the model to predict the most likely next word/s, given an input sentece (e.g., The dog is ...). If we remove the stopwords or we stem the words, the LM is not going to be able to learn from the actual input sentence (e.g., The dog is ... after stopword removal and lematization will be transformed to [dog, be])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jIV2NPE8xHd"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOGZ-GnU8xHf"
      },
      "source": [
        "We can now use the `data_lm` object I created earlier to fine-tune a pretrained language model. [fast.ai](http://www.fast.ai/) has an English model with an [AWD-LSTM architecture](https://arxiv.org/abs/1708.02182) available. To use it, we can create a learner object that will directly create the model, download the pretrained weights and be ready for fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76pJ6TVBXG1H"
      },
      "source": [
        "Neural nets in general, and the AWD_LSTM model in particular, are well know for having a huge number of hyperparameters to optimize.\n",
        "\n",
        "I am deliberately using the default values that fast.ai implements. fast.ai is well know for their superconvergent models that are able to train in just a few steps. This is done through their thoughtful research about initializations, regularization, optimizers and batch normalization. All of these findings are implmented in their default values, so, for most of the tasks, we can safely use them.\n",
        "\n",
        "There is though an hyperparameter that requires careful optimization, the learning rate. Learning rate refers to the \"speed\" at which the optimizer is updating the neural net parameters in the backpropagation step. Pick a large learning rate and your NN will never converge. Pick a small rate and you will wait forever to see your NN achive a good performance or to see your NN stuck at a local minima.\n",
        "\n",
        "I will make use of the `lr_find` method in fast.ai to make sense of the values that are optimal for my data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K34SbNbAbU_8"
      },
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3,\n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RBC55DQYluA"
      },
      "source": [
        "learn.lr_find(start_lr=1e-6, end_lr=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ZCjmtLZnmo"
      },
      "source": [
        "What we see in this figure is the projection of how the model will perform based on different learning rates.\n",
        "\n",
        "We can see that around 5e-01 the training starts to go south (i.e., the loss/error starts rocketing). The minimal loss is around 1e-1. Nevertheless, we have to choose a value that is approximately in the middle of the sharpest downward slope. Why?! Well, for a more intuitive explanation you can check this [blog post](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html). Basically, the reason is:\n",
        "\n",
        "> [...] the minimum value is already a bit too high, since we are at the edge between improving and getting all over the place. We want to go one order of magnitude before, a value that's still aggressive (so that we train quickly) but still on the safe side from an explosion.\n",
        "\n",
        "\n",
        "This is given as an indication by the LR Finder tool, so let's try 1e-2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVbMmZmg8xHg"
      },
      "source": [
        "learn.fit_one_cycle(5, 1e-2, moms=(0.8,0.7,0.8), div=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLC5O_ji8xHk"
      },
      "source": [
        "After just only one epoch we have a pretty good result (i.e., remember that we are still fine-tuning the language model, so guessing right about one third of times which is going to be the next word is a pretty good model). It seems that the default values of the library based on the super-convergence approach are just fine. So, I am not going to pay attention to them anymore. In a more developed solution, we could spend some time trying to optimize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl1vF5fscBMx"
      },
      "source": [
        "At this point it is important to explain what are we actually training. As I explained before, we have a pre-trained model of the English language. By pretrained, we mean nothing more the Neural Net has a set of weights already learned.\n",
        "This model is quite deep, it has many number of layers with millions of parameters (a.k.a. weights) to learn. It has been posible to train such a huge model, because the entire Wikipedia was used.\n",
        "\n",
        "However, we only have a small bunch of news reports to adapt/fine-tune the model to our domain. Fine-tuning is nothing more than modifying a little bit the weights of the model, so they reflect the particular relationships in our data. But, since our dataset is rather small, if we try to retrain all the layers in the model, we will completely destroy it (the model will catastrophically forget everything learned from Wikipedia).\n",
        "\n",
        "If you think about it, it makes sense, the deeper layers in the model are actually learning the basic aspects of the language, while shallow layers are in charge of more high-level relationships. We do not want to change the deeper layers. Basic language relationships are the same in our dataset than in the entire English language (e.g., verb tenses, subject-object relationships). What we actually want to modify is the last layers of the model in charge of learning the high level relationships (e.g. Obama does something). Well, that's exactly what the default training method in fast.ai is doing: It freezes the deeper layers so you do not modify them while you are fine-tuning your model. How convenient, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2tCZIx4pVR-"
      },
      "source": [
        "In the previous step, we have fine-tuned the LM to the particular content of our dataset. To do this, we just trained the last layers in our model to avoid forgetting everything. Now that our model is in a *stable* state (it has learned from our data without losing what learned from Wikipedia), we can further train our model by unfreeze all the layers.\n",
        "\n",
        "The rationale is the following: we can slightly change the weights of all the layers so even the most basic aspects of the language can be redefined according to our data (e.g. Napoleonic wars Wikipedia pages can learn the model that France<-->invade is a likely outcome; however, I do not expect to read anything about a French invasion in the HuffPost).\n",
        "\n",
        "It has been experimentally proven that this process can improve the performane of Deep Learning models (see this [Howard and Ruder's paper](https://arxiv.org/pdf/1801.06146.pdf) for more details). We have to be extremely careful with this process, otherwise we will ruin all our work. To that end, I have reduced my learning rate by an order of magnitude\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M535Cjn28xHm"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMCYWM-ZtBOH"
      },
      "source": [
        "Cool! Only 5 epochs and my model has been improved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4DYmf_G8xHs"
      },
      "source": [
        "To understand what our language model is learning, you can run the [`Learner.predict`](https://docs.fast.ai/basic_train.html#Learner.predict) method and specify the number of words you want it to guess."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60xLjz1D8xHu"
      },
      "source": [
        "TEXT = \"Haha South Tampa\"\n",
        "N_WORDS = 20\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n",
        "         for _ in range(N_SENTENCES)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi4kICl5J1Nr"
      },
      "source": [
        "print(\"\\n\".join(preds))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khzniEGX8xHz"
      },
      "source": [
        "It doesn't make much sense (we have a tiny vocabulary here and didn't train much on it) but note that it respects basic grammar (which comes from the pretrained model) while it has adapted the conversation to the content of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zUkM4xZuI4B"
      },
      "source": [
        "\n",
        "I could futher train or experiment with more hyperparameters, but since the language model is not the final outcome and the LM seems good enough, I will move on to the classification step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTQymGWCuOLX"
      },
      "source": [
        "We can save the model for later uses. We need to save the encoder (the part that given a textual content creates the representation) which is the only part of the model needed for classification in the next section (i.e., the other part is the decoder in charge of translating the textual representation back to the text again)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIVKMQ688xH0"
      },
      "source": [
        "learn.save_encoder('ft_enc_extended')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a38ilFdZ8xH4"
      },
      "source": [
        "### Building a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j90240MvKOHS"
      },
      "source": [
        "\n",
        "We now have the language model fine-tuned to our dataset. The next step is create and fine-tune the classification model.\n",
        "\n",
        "To recap, a language model predicts the next word of a document, so it doesn't need any external labels. The classifier, however, needs an annotated dataset\n",
        "containing the texts to classify and their labels (`disaster` and `not-disaster` in our case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAHNtQXXKKsw"
      },
      "source": [
        "dls_class = DataBlock(\n",
        "    blocks=(\n",
        "        TextBlock.from_df('text', # We take the textual information from the column \"text\" in the dataset\n",
        "                          seq_len=40, # Maximum tweet length\n",
        "                          vocab=dls_lm.vocab # We use the same vocabulary representation than the one used for the LM\n",
        "                          ),\n",
        "            CategoryBlock), # Because we will use these data for classification\n",
        "    get_x=ColReader('text'),\n",
        "    get_y=ColReader('target'),\n",
        "    splitter=RandomSplitter(0.2)).dataloaders(training_df, bs=64) # Randomly splits the dataset (20% for Validation, 80% for training)\n",
        "\n",
        "dls_class.show_batch(max_n=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaT8zAK08xH5"
      },
      "source": [
        "It's now time to actually create the classifier taking our fine-tuned encoder. For this step we need the `data_clas` object we created earlier.\n",
        "\n",
        "For this step, I will use again the default values in fast.ai to create a classification model. In this case, I will use the `text_classifier_learner`. We need to also tell the classifier that we want to use the fine-tuned encoder that we have just learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prrNRWVC8xH7"
      },
      "source": [
        "learn = text_classifier_learner(dls_class, AWD_LSTM, drop_mult=0.8, metrics=[accuracy,FBeta(beta=1)]).to_fp16()\n",
        "learn.load_encoder('ft_enc_extended')\n",
        "learn.freeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-F9CfhP430M"
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej0rANCquU09"
      },
      "source": [
        "Thanks to fast.ai super-convergence strategies, I think that just one epoch should be enough. Let's see"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou8W_Imd8xIJ"
      },
      "source": [
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io2s9yiImVAH"
      },
      "source": [
        "learn.save('lstm_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz7J-croud_D"
      },
      "source": [
        "Great! As I expected the accuracy has improved: 82%.\n",
        "\n",
        "This is in the line of the accuracy reported by the simpler models that I already implemented in the other markdown. However, we need to compare them in terms of the F1 macro AVG as we discussed. In additon we can futher train this model. As in the language modeling training, we have just trained the last layers of the model, we can gradually unfreeze the rest of the model and train it together.\n",
        "\n",
        "In this case I will not unfreeze all the model at a time, but in different steps. This gradual unfreezing has been experimentally proven to improve the training process (again, check Howard and Ruder's paper)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twCBr7mL8xIO"
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL-ALzPzmbwO"
      },
      "source": [
        "learn.save('lstm_2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzpVi542mle_"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(1e-2/2/(2.6**4),1e-2/2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Z_xXW_mpWG"
      },
      "source": [
        "learn.save('lstm_3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJVT_NlT8xIS"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(20, slice(1e-2/10/(2.6**4),1e-2/10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0moaKqhImyLI"
      },
      "source": [
        "learn.save('lstm_4')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02DIp2_ov37s"
      },
      "source": [
        "As can be seen in the results, our model is now offering accuracies up to 85%! Better than those we had with LR, RF or SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZTY2buWwMBx"
      },
      "source": [
        "In order to have a full report of this model performance, I will ask the model to offer me all the predictions and inspect the confusion matrix and the classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LglwOWs5iUuh"
      },
      "source": [
        "# get predictions\n",
        "preds, targets = learn.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "pd.crosstab(predictions, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PugeNbGkWF6"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(targets, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EE_Z3qRwfZ3"
      },
      "source": [
        "As can be seen in both the confusion matrix and in the classification report, the model is better not only in terms of accuracy, but also in its performance for all the classes when compared to the non Deep Learning models. Something that I care about since the start.\n",
        "\n",
        "As expected, there are some classes for which the model performs better (they are easier to predict or we have more data for them). However, we get a reasonable performance for most of the classes (I am a little bit worried by class 8, but since we do not have many data I guess I can accept this result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPE9ufJowcT2"
      },
      "source": [
        "Finally, saving the model object to reuse it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zdg1Ay2ho97"
      },
      "source": [
        "learn.save('final_lstm_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TacHyhsetfiI"
      },
      "source": [
        "# STEP 4: Transformers. Is attention all you need for Text Classification?\n",
        "\n",
        "In this third solution I want to go a step beyond to apply the most advanced methodologies in the NLP state-of-the-art: Transformers. We have already discussed Transformers in class, as well as their advantages over other methodologies.\n",
        "\n",
        "There is still some controversy about what technique is better suited for Text Classification (and NLP in general): RNNs or Transformers. Different research works suggest different experimental results. Nevertheless, it seems that Transformer-based models are starting to outperform RNN-based systems. Let's check what happens in our problem.\n",
        "\n",
        "Using Transformes might sound scary (many hyperparameters, you have to deal with tensor, shapes, transform your data....). However, there are a number of libraries and wrappers that will make your life easier (or at least less miserable). In particular, in this solution I will make use of two of them, which I strongly recommend you to use if you want to experiment with these ideas:\n",
        "\n",
        " - [HuggingFace](https://huggingface.co/): They provide under the [`transformers`](https://github.com/huggingface/transformers) library a large number of pre-trained state-of-the-art pretrained NLP models together with some handy functions to load them, fine-tune them and put them into practice.\n",
        " - [ktrain](https://github.com/amaiya/ktrain): Python library that provides a higher-level wrapper that facilitates the use of Deep Learning based NLP models (HuggingFace models among them).\n",
        "\n",
        "Let the fun begin!\n",
        "\n",
        "**Disclaimer:This solution is based on this [post](https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed) as well as in the [ktrain](https://github.com/amaiya/ktrain) library documentation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DStVzUwHmLXP"
      },
      "source": [
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
        "\n",
        "import tensorflow as tf\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmiEA41murJY"
      },
      "source": [
        "## Create the Transformer Model\n",
        "\n",
        "In this step I will take the dataset, preprocess it (via ktrain) and create a Text Classifier based on a Transformer model. This process follows a similar intuition to what we already did with fast.ai in the other markdown, although it is not exactly the same.\n",
        "\n",
        "We will start again with a Language Model pre-trained using a huge dataset: a model that speaks English. The model in particular is the well-know BERT. We already explained the basic rationale of BERT in class, but just as a reminder: it applies the idea of Self-Attention (instead of an RNN architecture) to learn the sequential information of textual contents. If you want to know more, let me refer you again to this great blog post: http://jalammar.github.io/illustrated-bert/.\n",
        "\n",
        "As we also did in the other markdown, we have to now fine-tune this model: Retrain the model to our specific dataset and task. In more detail, the BERT model provides a encoder block with pre-trained weights which gives BERT a general understanding of English. The BERT encoder block will look at each input tweet as a whole, producing an output that contains an understanding of the textual content of the tweet. This representation is then feed to a Text classifier. Thanks to ktrain, this process only involves a handful of lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQF4dgZLu39d"
      },
      "source": [
        "trn, val, preproc = text.texts_from_df(training_df,'text',preprocess_mode='bert',label_columns='target',verbose=True, maxlen=32) # Process the input tweets based on the BERT encoder\n",
        "\n",
        "model = text.text_classifier('bert', trn, preproc=preproc) # Create a text classifier that uses the BERT-based representations created before\n",
        "\n",
        "learner = ktrain.get_learner(model, train_data=trn, val_data=val,  batch_size=128) # Creates the learning process to fine-tune bert and train the classifier."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU9diVmz1vc8"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "It is now time to actually train (fine-tuning + train the text classifier) the model. This process might take a while. BERT is huge and memory-hungry. Consequently, it is pretty slow for both training and prediction.\n",
        "\n",
        "Since our dataset is not very large, you can train the model in a reasonable amount of time if you have a GPU at your disposal (if you don't just use Google Colab or another platform offering GPUs as I did. Do not try to train this model in your laptop if you do not want to potentially wait for hours).\n",
        "\n",
        "Nevertheless, because of this complexity, BERT is not very suitable for production environments. In this sense, I recommend you to take a look to DistilBERT. DistillBERT is a “distilled” (a.k.a. reduced) version of BERT that is smaller and faster while retaining most of BERT’s accuracy. Check this blog post from HuggingFace for more details on DistillBERT and the whole idea of Knowledge Distillation: https://medium.com/huggingface/distilbert-8cf3380435b5).\n",
        "\n",
        "There are many other pre-trained models that HuggingFace provides. You can check the entire Model zoo in the following link: https://huggingface.co/transformers/pretrained_models.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapnMAwc6lhn"
      },
      "source": [
        "The first thing to do is to find the optimal learning rate (as we did with our LSTM model). As you can see the ktrain interface is quite similar to the fast.ai interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD5f-vk21lXb"
      },
      "source": [
        "learner.lr_find(show_plot=True, max_epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsPkqT9l19rR"
      },
      "source": [
        "As we already now, we want a learning rate which is more or less in the middle of the steeper slope of the loss function. In our case is somewhere about 1e-4.\n",
        "\n",
        "I will train the model for a some of epochs and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gU-GYCt2PSK"
      },
      "source": [
        "learner.autofit(lr=1e-4, # Learning Rate\n",
        "                epochs=5, # Number of epochs to train the model\n",
        "                early_stopping=2, # If the model does not improve after 2 epochs, we stop the training\n",
        "                reduce_on_plateau=1,  # If the model does not improve aftear 1 epoch, we reduce the learning rate\n",
        "                monitor='val_loss', # Metric to monitor the peformance of the model (loss computed on the validation dataset)\n",
        "                checkpoint_folder='nlp_disaster/models/transformer_cpt_1' # After each epoch we store a checkpoint of the model\n",
        "                );"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek1waYfs2UfG"
      },
      "source": [
        "The first thing to notice is that the model takes a lot of time to train! Way more than the LSTM model. This is one of the donwsides of using a Transformer model. I only recommend you to use them if they do provide a significant improvement compared to the LSTM model. Sadly, this is not the case, under this configuration we are not able to improve the 0.85 accuracy obtained by the previous LSTM model.\n",
        "\n",
        "I do encourage you to try different paramaters (e.g., learning rate, model) to see if you can improve this performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4t8kmXzoMSR"
      },
      "source": [
        "learner.save_model('nlp_disaster/models/transformer_1') # Saving the trained model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO7p1LaQ8Eo3"
      },
      "source": [
        "We are going to now unfreeze the whole model to allow its retraining. As with the LSTM model, be careful! If you are too much aggressive with this re-training you will end up ruining the model.\n",
        "\n",
        "In this sense, I have reduced the learning rate by a magnitude order and try to retrain for one more epoch an check the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16scX2Z_2Ssu"
      },
      "source": [
        "learner.unfreeze()\n",
        "learner.fit_onecycle(1e-4/10, 1, checkpoint_folder='nlp_disaster/models/transformer_cpt_2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1v3aqCS8bJt"
      },
      "source": [
        "No apparent improvement. It seems that, at least for this case, LSTM models are preferred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqTOmFDloX8h"
      },
      "source": [
        "learner.save_model('nlp_disaster/models/transformer_2') # Store the final model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho6eSo9IcI3_"
      },
      "source": [
        "# STEP 5: Evaluate and Inspect the Model\n",
        "\n",
        "Large Deep Learning models are well-know for being quite black-boxy. However, in this section, I would like to provide you with some tools to evaluate and analyze them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvTuKoSmV0lY"
      },
      "source": [
        "First I will create the confussion matrix to understand the performance of the model and to check the accuracy and F-measure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvcxCvLOcOje"
      },
      "source": [
        "learner.validate(class_names=['Regular Tweet','Disaster Tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dx2wf7uV5xn"
      },
      "source": [
        "The model is a little bit better on detecting regular tweets; I guess that this is because we have more of those. Anyhow, it works pretty well for both classes in terms of accuracy and F-measure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhG3fPtPcVKe"
      },
      "source": [
        "If you want to better understand how the model is working ktrain provides some useful techniques.\n",
        "\n",
        "You can examine the validation tweets about which the model was the most wrong about (i.e., the top losses)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCABLebacTWM"
      },
      "source": [
        "learner.view_top_losses(n=10, preproc=preproc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43qyNa66Znba"
      },
      "source": [
        "ktrain, actually provides an `explain` method to know that. It allows you to inspect a given tweet and visualize which words contributed the most on deciding the final prediction. We will need a forked version of the **eli5** library that supportes TensorFlow Keras, so let's install it first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6HjLF9dd5iZ"
      },
      "source": [
        "!pip3 install -q git+https://github.com/amaiya/eli5@tfkeras_0_10_1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByXyFJY_bUrn"
      },
      "source": [
        "And call the `explain` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HgZDLYUeVaM"
      },
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc)\n",
        "\n",
        "predictor.explain(\"[CLS] the government is concerned about the population explosion and the population is concerned about the government explosion . - joe moore [SEP]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drtkzp8henR_"
      },
      "source": [
        "Thanks to this library and the `explain` method you can understand why the model is failing.\n",
        "\n",
        "For example, the tweet\n",
        "\n",
        "  `[CLS] the government is concerned about the population explosion and the population is concerned about the government explosion . - joe moore [SEP]`\n",
        "\n",
        "has been categorized as disaster when it is not about a disaster.\n",
        "\n",
        "We can understand that the problem is that the model is focusing on the word explosion, which is categorized as highly related to the disaster class. While it seems to be usually right, explosions are a strong indicators of disasters, in this situation the model is not able to understand the the word explosion is being used ironically."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Introduction to Zero-Shot Classification\n",
        "\n",
        "In this exercise, we delve into the realm of zero-shot text classification using a pre-trained language model. Zero-shot classification refers to the model's ability to accurately categorize text into specific classes without having been explicitly trained on those classes.\n",
        "\n",
        "## Exercise Overview:\n",
        "\n",
        "- **Objective**: To classify texts from our dataset as either related to an actual disaster or not, using a zero-shot approach.\n",
        "- **Model Utilized**: We are employing a model from the Hugging Face library, pre-trained on diverse datasets, enabling it to understand and infer the context and categorize texts appropriately.\n",
        "- **Method**: The model receives texts and candidate labels (\"Disaster\" and \"Not a Disaster\") and predicts the relevance of each label to the text.\n",
        "- **Evaluation**: We'll assess the model's performance against our dataset's actual labels, using metrics such as accuracy and F1 score to gauge its effectiveness.\n",
        "\n",
        "This exercise offers a practical application of zero-shot classification, showcasing how advanced NLP models can be leveraged for complex categorization tasks without the need for extensive task-specific training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G2Bzt0gBYu_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell illustrates the application of zero-shot classification using the `transformers` library, specifically leveraging the `facebook/bart-large-mnli` model. This model is an instance of BART (Bidirectional and Auto-Regressive Transformers) adapted for the MultiNLI (Multi-Genre Natural Language Inference) task. It excels in tasks requiring nuanced understanding of context and inference, making it ideal for zero-shot classification where the model must categorize texts into classes not seen during training.\n",
        "\n",
        "### Applying the Model:\n",
        "- **Pipeline Initialization**: We initialize the zero-shot classification pipeline with the BART model fine-tuned on the MNLI dataset. The pipeline is a high-level abstraction that wraps around the model and tokenizer, simplifying the task of text classification. It automatically handles the preprocessing of text and post-processing of model predictions\n",
        "- **Classification Task**: Our task is to classify texts as \"Disaster\" or \"Not a Disaster\". To that end, we just need to define what `labels` we want the model to use. This showcases the model's ability to apply learned inferences to new, unseen categories.\n",
        "\n",
        "Explore the model further on the [Hugging Face Model Hub](https://huggingface.co/facebook/bart-large-mnli)."
      ],
      "metadata": {
        "id": "SyaKN6IERbBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Example text (replace this with the text from your dataset)\n",
        "text = \"Two cranes restoring a bridge in the central Dutch city of Alphen aan den Rijn collapsed on houses.\"\n",
        "\n",
        "# Candidate labels for classifying whether the text is about an actual disaster\n",
        "labels = [\"Disaster\", \"Not a Disaster\"]\n",
        "\n",
        "# Perform classification\n",
        "result = classifier(text, labels)\n",
        "\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Labels: {result['labels']}\")\n",
        "print(f\"Scores: {result['scores']}\")\n"
      ],
      "metadata": {
        "id": "Vv-RJAAiY0Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will apply the previous pipeline to classify the data in our test dataset. As this is a rather slow process, I will subsample 1% of the data and apply it only to that. Feel free to extend it to the entire dataset.\n",
        "\n",
        "To simplify the application of the zero-shot-classification pipeline, I have created a wrapper function `classify_text`."
      ],
      "metadata": {
        "id": "qapPXjQ8Sc_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Assuming training_df is your DataFrame and it has a column named 'text'\n",
        "\n",
        "# Load the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Function to classify a single piece of text\n",
        "def classify_text(text):\n",
        "    labels = [\"Disaster\", \"Not a Disaster\"]\n",
        "    result = classifier(text, labels)\n",
        "    return result['labels'][0]\n",
        "\n",
        "# Wrap your pandas apply with tqdm for a progress bar\n",
        "tqdm.pandas(desc=\"Classifying\")\n",
        "\n",
        "sample_df = test_df.sample(frac=0.01)\n",
        "\n",
        "# Apply the classifier to each row in the DataFrame\n",
        "sample_df['predicted_label'] = sample_df['text'].progress_apply(classify_text)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "sample_df.head()\n"
      ],
      "metadata": {
        "id": "jDObSaLjY1N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we would like to evaluate the peformance of our model."
      ],
      "metadata": {
        "id": "6lO0R27DTJL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "\n",
        "# Map your actual labels to the same format as your predicted labels, if necessary\n",
        "# For example, if your actual labels are 0 and 1, map them to ['Not a Disaster', 'Disaster']\n",
        "label_mapping = {0: 'Not a Disaster', 1: 'Disaster'}\n",
        "sample_df['actual_label'] = sample_df['target'].map(label_mapping)\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(sample_df['actual_label'], sample_df['predicted_label'])\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate Precision, Recall, and F1 Score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(sample_df['actual_label'], sample_df['predicted_label'], average='binary', pos_label='Disaster')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZWYWF465aySg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, results are far from those achieved by previous methods. The main reasons are:\n",
        "\n",
        "\n",
        "*   The model that we are using is far from being the state of the art. In that sense, I recommend you to try more advanced models (e.g., LLaMA-2 or ChatGPT)\n",
        "*   We have not tried any prompt engineering technique. By refining the prompt, we will be able to explain the model exactly what we expect from it\n",
        "*   We can extend to few-shot learning, meaning we can provide the model with some annotated examples (e.g, tweets classified as Disaster or Not Disaster) to learn from\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hYBGHhzfhOa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, in this cell we will make use of an advanced LLM out of OpenAI's interface. The process is similar, but connecting to the OpenAI API via LangChain.\n",
        "\n",
        "Be aware, that in order to execute it, you need an OpenAI API KEY and pay for it ;)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oNIjpbP0FbSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) install and import\n",
        "!pip install --quiet openai langchain\n",
        "\n",
        "import os\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# 2) Configure your OpenAI key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n",
        "# 3) Define your labels and example\n",
        "labels = [\"Disaster\", \"Not Disaster\"]\n",
        "example_text = \"A sudden earthquake struck the city at dawn, collapsing buildings.\"\n",
        "\n",
        "# 4) Build the prompt\n",
        "template = \"\"\"\n",
        "Classify the following text into one of these labels: {labels}\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\n",
        "Answer with exactly one label.\"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"text\",\"labels\"], template=template)\n",
        "\n",
        "# 5) Create and run the chain\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.run(text=example_text, labels=\", \".join(labels))\n",
        "\n",
        "# 6) Display the result\n",
        "print(\"Input:     \", example_text)\n",
        "print(\"Prediction:\", result.strip())"
      ],
      "metadata": {
        "id": "z3wJGagWh8uW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}